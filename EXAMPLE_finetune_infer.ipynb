{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b23268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import src.training as training\n",
    "import src.models as models\n",
    "\n",
    "# Set device for PyTorch\n",
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 14\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a4e9cd",
   "metadata": {},
   "source": [
    "### Load training data\n",
    "\n",
    "- Emulator must be finetuned using a small sample of spectra - O(100) - from the target N(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078cffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data for finetuning the MAML model\n",
    "filepath = '../cl_ee_mcmc_dndz_nsamples=30000.h5'\n",
    "n_finetune = 100  # Number of training samples for finetuning\n",
    "\n",
    "train_data, test_data, ScalerY, ScalerX = training.load_train_test_val(\n",
    "    filepath=filepath, n_train=n_finetune, n_val=None, n_test=None, seed=seed,\n",
    "    device=device\n",
    ")\n",
    "X_train, y_train = train_data[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0992e",
   "metadata": {},
   "source": [
    "### Next we need to load the model and apply the trained weights\n",
    "\n",
    "- Parameters below are as configured for the paper, I haven't done extensive optimisation on them, feel free to play around with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd38100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = X_train.shape[1]\n",
    "out_size = y_train.shape[1]        \n",
    "\n",
    "# Construct model architecture\n",
    "model = models.FastWeightCNN(\n",
    "    input_size=in_size,\n",
    "    latent_dim=(16,16),\n",
    "    output_size=out_size,\n",
    "    dropout_rate=0.2\n",
    ")\n",
    "\n",
    "# Initialise a MetaLearner for training\n",
    "metalearner = training.MetaLearner(\n",
    "    model=model,\n",
    "    outer_lr=0.01, # Training params\n",
    "    inner_lr=0.001,\n",
    "    loss_fn=torch.nn.MSELoss,\n",
    "    beta1=0.9, # Adam params\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-8,\n",
    "    seed=seed, # Random seed for reproducibility\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load an apply the trained meta-weights\n",
    "weight_path = 'weights\\WEIGHTS_5batch_500samples_20tasks_14seed.pt'\n",
    "metalearner.model.load_state_dict(\n",
    "    torch.load(weight_path, map_location=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d123e9",
   "metadata": {},
   "source": [
    "### Now we finetune the model\n",
    "\n",
    "- Task specific weights are stored separately and passed at runtime so as to protect the meta-weights from being overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c68ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_epochs = 64\n",
    "\n",
    "fast_weights, _ = metalearner.finetune(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    adapt_steps=finetune_epochs,\n",
    "    use_new_adam=True # Start with fresh adam optimizer state for finetuning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68840ad6",
   "metadata": {},
   "source": [
    "### Finally, we can test the trained model on the rest of the sample spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the test data to a torch dataloader\n",
    "# Can modify batch size as needed depending on system resources\n",
    "test_loader = DataLoader(test_data, batch_size=5000, shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "metalearner.model.eval()\n",
    "y_pred = torch.tensor([]).to(device)\n",
    "for X_batch, y_batch in test_loader:\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # don't compute gradients during inference\n",
    "        y_pred_batch = metalearner.model(X_batch, params=fast_weights)\n",
    "        y_pred = torch.cat((y_pred, y_pred_batch), dim=0)\n",
    "\n",
    "print('Total predictions:', y_pred.shape)\n",
    "# Inverse transform the data\n",
    "y_pred = ScalerY.inverse_transform(y_pred)\n",
    "\n",
    "y_pred_np = y_pred.cpu().numpy()\n",
    "y_test = test_data[1]  # Get original test data\n",
    "y_test_np = y_test.cpu().numpy()\n",
    "\n",
    "# Exponentiate the data\n",
    "y_pred_np = np.exp(y_pred_np)\n",
    "y_test_np = np.exp(y_test_np)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
